 ---
license: Apache License 2.0
text:
  conversational:
    size_scale:
      - 10k-1m
    type:
      - kg-qa
  question-answering:
    language:
      - zh

---

## 数据集描述
 
欢迎踏入“多元知识宝典”的殿堂，这里是您的私人炼金术士专属原料库！

🚀🌟想象一下，当美食、城市、企业家、汽车、明星八卦、生活常识、日常对话交织在一起，将诞生怎样一个神奇的结晶？

正是我们精心打造的——“多元知识宝典”！

🌈 这里有历史长河的波澜壮阔；

📱生活百科的点点滴滴；

💰商业精英的智慧火花；

✨明星八卦的幕后故事；

🍽️美食地图的味蕾盛宴；

🍎水果百科的甜蜜诱惑；

🏙️城市导览的奇幻旅程…… 

“多元知识宝典”远不止是一个数据集合，它是开启无限潜能的钥匙！

📚无论您旨在构建聊天机器人，还是希望通过语料库掌握seq2seq或Transformer技术，

这个拥有10万条精选中文对话的宝库都将为您提供助力。

🧙‍♂️炼丹大师们，是时候展现您精湛的技艺了！
 
让我们共同探索数据的奥秘，挖掘知识的瑰宝，为人工智能注入生命的活力。
 
记住，每一位卓越的炼丹师背后，都有一部值得信赖的宝典。

🎉启程吧，您的炼金之旅已然开启！愿您在数据的浩瀚海洋中，扬帆远航，早日成为掌控AI的炼丹大师！


## 语料库膨胀日志

2024.7.11  2800条对话

2024.7.14  3961条对话

2024.7.17  4813条对话

2024.7.21  5331条对话

2024.7.22  6028条对话

2024.8.15  13417条对话

2024.8.19  30000条对话

2024.8.20  41669条对话

2024.8.21  51000条对话

2024.8.22  63000条对话

2024.8.23  76950条对话

2024.8.24  90000条对话

2024.8.25  124000条对话

2024.8.27  150000条对话

2024.10.12 190000条对话

2024.10.14 270000条对话
## 数据集说明

为了强化模型对人类语言语义的深刻理解和提高其在面对未知数据时的表现，我们在构建数据集的过程中采取了一种策略，

即刻意纳入了一系列语义等价但表达形式各异的询问实例。这一策略的核心目的在于训练模型识别并掌握自然语言的多态性，从而显著增强其泛化能力。


比如

{"prompt": "我打算去旅行", "completion": "旅行是件美好的事情，你计划去哪里？有没有什么特别想去的地方？", "history": []}

{"prompt": "我想去旅行", "completion": "旅行是件美好的事情，你计划去哪里？有没有什么特别想去的地方？", "history": []}

{"prompt": "我计划去旅游", "completion": "旅游是件美好的事情，你计划去哪里？有没有什么特别想去的地方？", "history": []}


💡 想象一下，如果你的智能助手不仅听得懂你的“明话”，还能听懂你的“暗语”，那该有多酷啊！

比如，当你问：“今天外面冷不冷啊？”和“出门要不要穿羽绒服？”虽然字面上不一样，但潜台词都是想知道天气状况，对吧？

🌈 所以，在我们的“智慧宝典”数据集中，我们故意收集了一些问题，它们就像双胞胎一样，长得差不多，骨子里的意思却是一模一样的！

**但却不是冗余，反而可以有效增加模型泛化能力！**


🎯 这样一来，无论你怎么切换各种各样的表达方式，我们的模型都能get到你的点，知道你的真正意图。

这就好比你和一个老朋友聊天，不用说太多，对方就能懂你的意思，这种默契感，简直不要太棒！

🚀 所以，亲爱的朋友们，正是这些看似重复的问题，让我们的模型练就了一身“读心术”的本领，无论你怎么问，

它都能给出最贴切的回答。这就是我们数据集的魅力所在——让每一次对话，都像和老友谈心一样自然流畅！

## 数据集支持的任务

用来构建小型聊天机器人，或者用于学习seq2seq，以及Transformer技术使用。



## 其它相关信息

有任何问题，可以在交流反馈区与作者联系。